Данный проект предназначен для сбора доменов с "желтых страниц интернета"
Для добавления нового сборщика доменов необходимо:
- в папку url_lists_for_map поместить файл с расширением ".csv" в названии которого в начале имени должен присутствовать паттерн "word_"
- в папку crawlers поместить файл с написанным вами парсером. Он должен называться "word_parser.py"
- "word" - некое уникальное слово, которого еще нет среди парсеров в папке и которое как-то идентифицирует ваш парсер
- парсер обязательно должен содержать, как точку входа функицю с именем "parse_all", которая принимает два параметра:
  - дата фрейм с урлами для парсинга и метками категорий для того, чтобы смапать домены найденные по этому урлу.
  - имя временной папки, в которую вы будете писать промежуточные файлы. В дальнейшем цетральный блок кода будет брать из этой папки все файлы и собирать их в один для отправки на форест
