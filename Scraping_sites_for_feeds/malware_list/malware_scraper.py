import re
import requests
import tldextract
import logging
from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime as dt
import pandas as pd
from schemes import SCHEME

logging.basicConfig(filename="scrapErr", level=logging.INFO)
log = logging.getLogger()

# Secondary functions

PATH = 'https://forest1.getaura.ru/feeds/denis-feeds/'


def normDom(urlList, r_type='domain'):
    '''This function get domain from url
    Parameters: urlList - list object with string url
                r_type:"domain"/"url" - type of return data
    return: list of unique domain or url'''
    normList = []
    for url in urlList:
        dom = tldextract.extract(url)
        if dom.domain == '' or dom.suffix == '': continue
        if r_type == 'url':
            normList.append(url)
        else:
            dom = re.sub('www.', '', '.'.join(dom))
            normList.append(dom.strip('.'))
    return list(set(normList))


def update(newlist, oldlist, myfile, path=PATH):
    '''This function checks whether updates for the file have appeared and,
    if so, creates a new file with updates and puts them at the specified address.
    Parameters: newlist - list with new domain from site
                oldlist - list with old domain from myfile from path
                myfile - name file which must be updates
                path - path where the myfile is '''
    
    setNew = set(newlist)
    setOld = set(oldlist)
    
    '''if diff exist - update file'''
    if len(setNew.symmetric_difference(setOld)) > 0:
        df = pd.DataFrame(newlist, columns=['domain'])
        df.to_csv(myfile, sep='\t', index=False, header=False)
        resp = requests.put(path+myfile, data=open(myfile, 'rb'))
        if not resp.ok:
            log.error(resp.text)
        else:
            log.info("The {} successfully updated!".format(myfile))
    else:
        log.error("The {} has no updates today!".format(myfile))

    
def filter_df(df, column, filtr, typ=None):
    '''This function filters df by condition
    Parameters: df - input df
                column - name column for filtering
                filtr - function for filtering
                typ - type of column, if column requires transformation
    return: new df after filtering'''
    if not typ is None:
        '''if type exist - transformation to this type'''
        df[column] = df[column].astype(typ)
    df = df[df[column].apply(filtr)]
    return df


def text_parser(bsObj, split='\n', line=0, start_line=None, end_line=None, split_line=None, column=[],
               filtr=[], typ=[], ret=0, r_type='domain'):
    '''This function parse text from BeautifullSoap object
    Parameters: bsObj - input BeautifullSoap object
                split - sign for split text on line
                line - 0: not split the line; 1: split line
                start_line - line with which we start reading
                end_line - line with which we end reading
                split_line - sign for split line on column
                column - list with name column for filtering
                filtr - function for filtering
                typ - list with types for transformation
                ret = number of column for return
                r_type:"domain"/"url" - type of return data
    return: normalized domain (url) list'''
    '''If not filter - return normalized domain list'''
    if not line:
        return normDom(bsObj.get_text().split(split)[start_line:end_line])
    '''make list of lists from text. Split on line and then split on column'''
    lines = [line.split(split_line) for line in bsObj.get_text().split(split)[start_line:end_line]]
    df = pd.DataFrame(lines, columns=None)
    '''There may be blank lines. Remove them'''
    df = df.dropna()
    '''Filter'''
    for col, filt, t in zip(column, filtr, typ):
        df = filter_df(df, col, filt, t)
     
    return normDom(df[ret], r_type)


def makebs(url):
    '''This function create BeautifullSoup object from url
    Parameters: url - string object like "https://pep8.ru/doc/pep8/"
    return: BeautifullSoup object'''
    
    try:
        html = urlopen(url)
    except:
        log.error('URL {} could not open'.format(url))
        return
    return BeautifulSoup(html, features="lxml")


def constructor(bs, find, pos, var, **kwargs):
    '''This function looking for a new BeautifullSoap object according to the scheme  
    Parameters: bs - input BeautifullSoap object
                find - dict with parameters to search
                pos - position of tag in tags collection
                var - flag defining the type of search
                **kwargs - dict of not used parameters
    return: new BeautifullSoap object after finding the desired tag'''
    if var == 'findall':
        return bs.findAll(**find)[pos]
    elif var == 'findnext':
        return bs.findNextSiblings(**find)[pos]
    elif var == 'findprevious':
        return bs.findPreviousSiblings(**find)[pos]


def make_url(base_url, href):
    '''This function make new url from base_url and href
    Parameters: base_url - start url
                href - dynamic url or part of url
    return: new url for parsing'''
    if href.startswith('http://') or href.startswith('https://'):
        return href
    if not href.startswith('/'):
        return base_url + '/' + href
    base_url = base_url.split('/')
    base_url = base_url[0] + '//' + base_url[2]
    return base_url + href
    

def parse_malware(base_scheme, child_scheme, text_scheme):
    '''This function parse site. Looks for dynamic links if they exist. And then parses the lists of domains.
    Parameters: base_scheme - base scheme with instruction to parse site
                child_scheme - put in instruction to parse site
                text_scheme - text scheme with instruction to parse text information
                
    result: update file on https://forest1.getaura.ru/feeds/denis-feeds/'''
    for scheme, child, text in zip(base_scheme, child_scheme, text_scheme):
        
        log.info("Malware parser on url {} started in {}!".format(scheme['base_url'], dt.datetime.now()))
        if scheme['base_url'] != '':
            url = scheme['base_url']
        else:
            if 'add_url' in locals():
                url = add_url
            else:
                log.info("add_url used before declare. Check scheme.py: {}".format(scheme))
                return
            
        bs = makebs(url)
        if bs is None: return []
        log.info("bsObject successfully created for {}!".format(url))
        
        '''create list of bsObjs just in case there are several blocks on the page 
        for parsing lists of domains'''
        bs_list, bs_main = list(), bs
        for ch in child:
            '''implement instructions for finding tags'''
            try:
                bs = constructor(bs, **ch)
            except IndexError:
                bs = None
                
            if bs is None:
                log.error("Nothing was found under this scheme: {}!".format(ch))
                return []
            '''if child scheme has flag end_part==1, add bsObj to list and return to main bsObj of this page'''
            if ch['end_part']:
                bs_list.append(bs)
                bs = bs_main
        
        if scheme != base_scheme[-1]:
            if 'href' in bs_list[0].attrs:
                add_url = make_url(scheme['base_url'], bs_list[0]['href'])
            else:
                log.error("Not found dynamic link on {}!".format(url))
        else:
            try:
                oldlist = pd.read_csv(PATH+scheme['filename'], header=None)[0].values
            except:
                '''if file not found - create empty list. It will be write to new file'''
                oldlist = []
                log.info("Not found file {}! So create new file.".format(PATH+scheme['filename']))
            domain = list()
            '''for all bsObj: parse it and add result to one list'''
            for bsObj in bs_list:
                domain.extend(text_parser(bsObj, **text))
            '''update file or create it if not exist'''
            log.info("Updated {} started!".format(scheme['filename']))
            update(domain, oldlist, scheme['filename'])

            
# Start
def main():
    '''implement all instructions from schemes'''
    log.info("Task started successfully!")
    for scheme in SCHEME:
        parse_malware(*scheme)
    log.info("Task completed successfully!")

if __name__ == '__main__':
    exit(main())
