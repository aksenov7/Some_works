{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, TfidfModel, Doc2Vec, FastText\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bokeh\n",
    "import bokeh.models as bm, bokeh.plotting as pl, bokeh.palettes as palettes\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vectors(x, y, radius=10, alpha=0.25, classes=None,\n",
    "                 width=600, height=400, show=True, tokens=None):\n",
    "    \n",
    "    if classes is None:\n",
    "        color = [\"blue\"] * len(x)\n",
    "    else:\n",
    "        n = np.unique(classes)\n",
    "        palette = palettes.viridis(len(n))\n",
    "        indx = {n[i]:i for i in range(len(n))}\n",
    "        color = [palette[indx[i]] for i in classes]\n",
    "    \n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    if classes is None:\n",
    "        data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, \"tokens\": tokens })\n",
    "    else:\n",
    "        data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, 'classes':classes, \"tokens\": tokens })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    if classes is None:\n",
    "        fig.circle('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "    else:\n",
    "        fig.circle('x', 'y', size=radius, color='color', alpha=alpha, source=data_source, legend='classes')\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(\"tokens\", \"@\" + \"tokens\")]))\n",
    "    \n",
    "    url = \"https://@tokens\"\n",
    "    fig.add_tools(bm.TapTool(callback=bm.OpenURL(url=url)))\n",
    "    if not classes is None:\n",
    "        fig.legend.location = \"top_left\"\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load('700K/gensim_lda_model_700K_400_de8')\n",
    "lsi = LsiModel.load('700K/gensim_lsi_model_700K_de8')\n",
    "doc2vec = Doc2Vec.load('700K/doc2vec_model_700K_de')\n",
    "fast = FastText.load('700K/fasttext_700K_de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_size = lda.num_topics\n",
    "lsi_size = lsi.num_topics\n",
    "doc2vec_size = doc2vec.vector_size\n",
    "fast_size = fast.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatify(pair_list, size):\n",
    "    result = [0. for _ in range(size)]\n",
    "    for (idx, val) in pair_list:\n",
    "        result[idx] = float(val)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('de_inter_shop.csv', sep='\\t')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import  word_tokenize\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "\n",
    "stemm = Cistem()\n",
    "tokk = RegexpTokenizer('\\-?[0-9]+(?:\\.[0-9]+)?|\\w+')\n",
    "# sw = stop_words.get_stop_words(\"de\")\n",
    "\n",
    "def stemm_it(t):\n",
    "    ll = tokk.tokenize(t)\n",
    "    #ll = [x for x in ll if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    return [i for i in map(stemm.stem, ll)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content = df.content.apply(lambda x: x.lower())\n",
    "df.content = df.content.apply(lambda x: re.sub('[0-9]', ' ', x))\n",
    "df.content = df.content.apply(lambda x: re.sub(r's+', ' ', x))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dct = Dictionary.load_from_text(\"700K/de_dict_700K.dict\")\n",
    "tfidf = TfidfModel.load(\"700K/tfidfModel_700K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = df.content.apply(lambda x: x.split())\n",
    "tokens = df.content.apply(stemm_it)\n",
    "d2b_vector = [loaded_dct.doc2bow(token) for token in tokens]\n",
    "tf_vector = tfidf[d2b_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tf_vectors'] = tf_vector\n",
    "df['tokens'] = tokens\n",
    "df['content'] = df.tokens.apply(lambda r: ' '.join(r))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector_lda'] = df.tf_vectors.apply(lambda x: flatify(lda[x], lda_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector_lsi'] = df.tf_vectors.apply(lambda x: flatify(lsi[x], lsi_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fast'] = df.content.apply(lambda x: fast.wv[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc2vec'] = df.tokens.apply(lambda x: doc2vec.infer_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['clusters_lda', 'clusters_lsi', 'clusters_d2v', 'clusters_fast'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('de_int_shop_cluster.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_lda = df.vector_lda.apply(pd.Series).values\n",
    "vector_lsi = df.vector_lsi.apply(pd.Series).values\n",
    "vector_fast = df.fast.apply(pd.Series).values\n",
    "vector_doc2vec = df.doc2vec.apply(pd.Series).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "lda_pic = TSNE(n_components=2).fit_transform(vector_lda)\n",
    "lsi_pic = TSNE(n_components=2).fit_transform(vector_lsi)\n",
    "fast_pic = TSNE(n_components=2).fit_transform(vector_fast)\n",
    "doc2vec_pic = TSNE(n_components=2).fit_transform(vector_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer1 = umap.UMAP()\n",
    "lda_umap = reducer1.fit_transform(vector_lda)\n",
    "reducer2 = umap.UMAP()\n",
    "lsi_umap = reducer2.fit_transform(vector_lsi)\n",
    "reducer3 = umap.UMAP()\n",
    "fast_umap = reducer3.fit_transform(vector_fast)\n",
    "reducer4 = umap.UMAP()\n",
    "doc2vec_umap = reducer4.fit_transform(vector_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def de_clusters(type_clust ,vectors, num_clusters, name_vec, vec_for_draw):\n",
    "    if type_clust == 'aglo':\n",
    "        agglo = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean')\n",
    "        answer = agglo.fit_predict(vectors)\n",
    "    elif type_clust == 'kmeans':\n",
    "        km = KMeans(n_clusters=num_clusters)\n",
    "        km.fit(vectors)\n",
    "        answer = km.labels_.tolist()\n",
    "    df['clusters_{}'.format(name_vec)] = answer\n",
    "    data = df['domain']\n",
    "    x, y = vec_for_draw[:,0], vec_for_draw[:,1]\n",
    "    color = answer\n",
    "    draw_vectors(x,y, classes=color, alpha=0.7, tokens=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "type_clust = 'kmeans'\n",
    "zip_param = [\n",
    "    [type_clust, vector_lda, num_clusters, 'lda', lda_umap],\n",
    "    [type_clust, vector_lsi, num_clusters, 'lsi', lsi_umap],\n",
    "    [type_clust, vector_fast, num_clusters, 'fast', fast_umap],\n",
    "    [type_clust, vector_doc2vec, num_clusters, 'd2v', doc2vec_umap],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zip_param:\n",
    "    de_clusters(*z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
